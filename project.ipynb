{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for segment in segments:\\n    print(segment, end='\\n') \""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, read in file\n",
    "\n",
    "with open(\"Texts-UTF8.txt\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# print(contents)\n",
    "\n",
    "# now, split contents into useful segments\n",
    "\n",
    "segments = re.findall(r\"\\\\tx.*?(?=\\n\\\\ref)\", contents, re.DOTALL)\n",
    "# looks for matches starting with \\tx and ending with the next file's name\n",
    "# no need to filter out irrelevant data with this form of matching\n",
    "\n",
    "\"\"\" for segment in segments:\n",
    "    print(segment, end='\\n') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for element in file_data:\\n    for items in element.items():\\n        print(items, end='\\n')\\n    print('\\n')  \""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function that converts segments into iterables\n",
    "\n",
    "def convert(segment):\n",
    "    lines = segment.split('\\n')\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "            morphemes = line.split()\n",
    "            line = ' '.join(morphemes)\n",
    "            line = line.replace(\"- \", \"-\").replace(\"= \", \"=\").replace(\" -\", \"-\").replace(\"\\\\\", \"\")\n",
    "            if line != '':\n",
    "                new_lines.append(line)\n",
    "\n",
    "    lines_dict = {}\n",
    "    for line in new_lines:\n",
    "        spline = line.split()\n",
    "        lines_dict[spline[0]] = spline[1:]\n",
    "    \n",
    "    return lines_dict\n",
    "\n",
    "file_data = [] # this will be a list of dictionaries, with each dictionary representing one conversation segment\n",
    "\n",
    "for segment in segments:\n",
    "    data = convert(segment)\n",
    "    data.pop('mb', None)\n",
    "    if data['tx'] != ['EMPTY']: # filter out empty data\n",
    "        for i in range(len(data['tx'])):\n",
    "            data['tx'][i] = data['tx'][i].lower()\n",
    "            data['ps'][i] = data['ps'][i].lower()\n",
    "        file_data.append(data)\n",
    "\n",
    "# print out contents of file_data just to check\n",
    "\"\"\" for element in file_data:\n",
    "    for items in element.items():\n",
    "        print(items, end='\\n')\n",
    "    print('\\n')  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions for search function(s)\n",
    "\n",
    "def generate_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "def print_search_results(element):\n",
    "    translation = \" \".join(element.get(\"ft\", []))\n",
    "    element_copy = element.copy()\n",
    "    keys = ['tx', 'ph', 'ge', 'ps']\n",
    "    max_len = max([len(element.get(key, [])) for key in keys])\n",
    "    for key in element_copy.keys():\n",
    "        if key not in keys:\n",
    "            element.pop(key, None)\n",
    "        else:\n",
    "            if max_len > len(element.get(key, [])):\n",
    "                for i in range(max_len - len(element.get(key, []))):\n",
    "                    element[key].append('')\n",
    "\n",
    "    # now, create pd dataframe\n",
    "    # pandas allows us to easily identify and align each index with one another\n",
    "    df = pd.DataFrame(element)\n",
    "    df = df.transpose()\n",
    "    df.rename({'tx': 'Text:', 'ph': 'IPA:', 'ge': 'Gloss:', 'ps': 'POS:'}, axis='index', inplace=True)\n",
    "    print(df, '\\n')\n",
    "    print(\"Trans:\\t\"+translation)\n",
    "    print('\\n' + '='*100 + '\\n')\n",
    "\n",
    "\n",
    "# these search functions check if the input token is present in a list of n-grams of element\n",
    "# for any instance of the token within the list of n-grams, \n",
    "# a list of indexes denoting the position of the token within the n-gram list is returned\n",
    "# then, in the main search() function, these respective indexes can be compared to see if they match up with each other\n",
    "\n",
    "def searchByText(element, text_tokens):\n",
    "    text_indexes = []\n",
    "    text_ngram = generate_ngrams(element['tx'], len(text_tokens))\n",
    "    for index, value in enumerate(text_ngram):\n",
    "        if value == text_tokens:\n",
    "            text_indexes.append(index)\n",
    "    return text_indexes\n",
    "        \n",
    "def searchByGloss(element, gloss_tokens):\n",
    "    gloss_indexes = []\n",
    "    gloss_ngram = generate_ngrams(element['ge'], len(gloss_tokens))\n",
    "    for index, value in enumerate(gloss_ngram):\n",
    "        if value == gloss_tokens:\n",
    "            gloss_indexes.append(index)\n",
    "\n",
    "    return gloss_indexes\n",
    "\n",
    "def searchByPos(element, pos_tokens):\n",
    "    pos_indexes = []\n",
    "    pos_ngram = generate_ngrams(element['ps'], len(pos_tokens))\n",
    "    for index, value in enumerate(pos_ngram):\n",
    "        if value == pos_tokens:\n",
    "            pos_indexes.append(index)\n",
    "    \n",
    "    return pos_indexes\n",
    "\n",
    "def searchByPhono(element, phono_tokens):\n",
    "    phono_indexes = []\n",
    "    phono_ngram = generate_ngrams(element['ph'], len(phono_tokens))\n",
    "    for index, value in enumerate(phono_ngram):\n",
    "        if value == phono_tokens:\n",
    "            phono_indexes.append(index)\n",
    "\n",
    "    return phono_indexes\n",
    "\n",
    "def search(file_data, text, phono, gloss, pos):\n",
    "    sys.stdout = open('output.txt','wt') # writes output to a file\n",
    "    # tokenize inputs\n",
    "    matches = []\n",
    "\n",
    "    text_tokens = tuple(text.split())\n",
    "    gloss_tokens = tuple(gloss.split()) \n",
    "    pos_tokens = tuple(pos.split())\n",
    "    phono_tokens = tuple(phono.split())\n",
    "\n",
    "    token_categories = [text_tokens, gloss_tokens, pos_tokens, phono_tokens]\n",
    "    nonEmptyCategories = 0\n",
    "    num_tokens = set()\n",
    "    for token_category in token_categories:\n",
    "        if token_category:\n",
    "            nonEmptyCategories += 1\n",
    "            num_tokens.add(len(token_category))\n",
    "\n",
    "    if len(num_tokens) > 1:\n",
    "        print(\"Error: number of tokens in non-empty categories are not the same.\")\n",
    "        return []\n",
    "    elif len(num_tokens) == 0:\n",
    "        print(\"Error: There should be at least one non-empty search category.\")\n",
    "        return []\n",
    "    else:\n",
    "        for element in file_data:\n",
    "            indexes = []\n",
    "            if text_tokens:\n",
    "                text_indexes = searchByText(element, text_tokens)\n",
    "                indexes.append(text_indexes)\n",
    "            if gloss_tokens:\n",
    "                gloss_indexes = searchByGloss(element, gloss_tokens)\n",
    "                indexes.append(gloss_indexes)\n",
    "            if pos_tokens:\n",
    "                pos_indexes = searchByPos(element, pos_tokens)\n",
    "                indexes.append(pos_indexes)\n",
    "            if phono_tokens:\n",
    "                phono_indexes = searchByPhono(element, phono_tokens)\n",
    "                indexes.append(phono_indexes)\n",
    "            \n",
    "            if nonEmptyCategories == 1:\n",
    "                # just need to check if indexes list has non empty list\n",
    "                if indexes[0]:\n",
    "                    print_search_results(element)\n",
    "                    matches.append(element)\n",
    "            elif nonEmptyCategories > 1:\n",
    "                # check that each non-empty list within indexes have overlaps\n",
    "                    intersection = set(indexes[0])\n",
    "                    for sublst in indexes[1:]:\n",
    "                        intersection.intersection_update(sublst)\n",
    "                    if intersection:\n",
    "                        print_search_results(element)\n",
    "                        matches.append(element)\n",
    "\n",
    "    if not matches:\n",
    "        print(\"No matches.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this search works on multiple words, in all categories\n",
    "# just make sure that the number of words in each category is the same\n",
    "# unless you are intentionally leaving that category blank\n",
    "\n",
    "# 'run all' instead of running this single cell again whenever you change the search parameters\n",
    "# if not the translation will go missing\n",
    "\n",
    "# sample outputs (filenames):\n",
    "# output_1: \n",
    "# gloss: village\n",
    "# pos: n\n",
    "\n",
    "# output_2:\n",
    "# gloss: LOC village\n",
    "# pos: case n\n",
    "\n",
    "# output_3:\n",
    "# phono: Ë€i ki\n",
    "# pos: pn pn\n",
    "\n",
    "# output_4:\n",
    "# pos: pn pn\n",
    "\n",
    "text = ''\n",
    "phono = ''\n",
    "gloss = ''\n",
    "pos = ''\n",
    "\n",
    "search(file_data, text, phono, gloss, pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
